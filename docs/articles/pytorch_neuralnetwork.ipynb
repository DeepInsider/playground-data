{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch-neuralnetwork.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepInsider/playground-data/blob/master/docs/articles/pytorch_neuralnetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZhsEJSQM7Jh",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4JztMOxNIvu",
        "colab_type": "text"
      },
      "source": [
        "# 連載『PyTorch入門』のノートブック（1）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FteEuY5_NVhO",
        "colab_type": "text"
      },
      "source": [
        "<table valign=\"middle\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.atmarkit.co.jp/ait/subtop/features/di/pytorch_index.html\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/manabu.svg\"/>Deep Insiderで記事を読む</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/DeepInsider/playground-data/blob/master/docs/articles/pytorch_neuralnetwork.ipynb\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/gcolab.svg\" />Google Colabで実行する</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/DeepInsider/playground-data/blob/master/docs/articles/pytorch_neuralnetwork.ipynb\"> <img src=\"https://re.deepinsider.jp/img/ml-logo/github.svg\" />GitHubでソースコードを見る</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ueKLtX95oVW",
        "colab_type": "text"
      },
      "source": [
        "# 第1回　難しくない！　PyTorchでニューラルネットワークの基本"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_E3IdevRyna",
        "colab_type": "text"
      },
      "source": [
        "## ■PyTorchとは？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQwqGYlhR7-J",
        "colab_type": "text"
      },
      "source": [
        "- 人気急上昇中（参考：「[PyTorch vs. TensorFlow、ディープラーニングフレームワークはどっちを使うべきか問題 (1/2)：気になるニュース＆ネット記事 - ＠IT](https://www.atmarkit.co.jp/ait/articles/1910/31/news028.html)）」\n",
        "- Pythonic（＝Pythonのイディオムをうまく活用した自然なコーディングが可能）\n",
        "- 柔軟性や拡張性に優れる（特に“define-by-run”：実行しながら定義／eager execution：即時実行なので、例えばモデルのフォワード時にif条件やforループなどの制御フローを書いて動的に計算グラフを変更したりできる）\n",
        "\n",
        "特にNLP（Natural Language Processing：自然言語処理）の分野では、研究者はさまざまな長さの文を訓練する必要性があるため、動的な計算グラフが不可欠。実際に「PyTorchがデファクトスタンダードになっている」と筆者が初めて聞いたのは、NLPに関しての話だった。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJdujIPyYdDT",
        "colab_type": "text"
      },
      "source": [
        "## ■本稿の目的と方針"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Grrfk74Yl7a",
        "colab_type": "text"
      },
      "source": [
        "PyTorchでニューラルネットワークを定義するための最重要の基礎知識を最短で紹介する。\n",
        "\n",
        "- PyTorchのチュートリアルは最初からCNNで最初から複雑（※これはある程度のニューラルネットワークの知識がない人を門前払いする意味があると思う）\n",
        "- まずはニューラルネットワークの原型「ニューロン」を実装することで、核となる機能を理解する\n",
        "- 「ニューロン」「活性化関数」「正則化」「勾配」「確率的勾配降下法（SGD）」といった概念が分からない場合は、『[TensorFlow 2＋Keras（tf.keras）入門 - ＠IT](https://www.atmarkit.co.jp/ait/subtop/features/di/tf2keras_index.html)』の第1回～第3回で挙動を示しながら分かりやすく説明しているので、先にそちらを一読してほしい\n",
        "- 最終的には、基本的なニューラルネットワーク＆ディープラーニングのコードが思いどおりに書けるようになる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzMyXEPUNmKU",
        "colab_type": "text"
      },
      "source": [
        "## ■本稿で説明する大まかな流れ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFOYuPJ6Ngnu",
        "colab_type": "text"
      },
      "source": [
        "- （1）ニューロンのモデル定義\n",
        "- （2）フィードフォワード（順伝播）\n",
        "- （3）バックプロパゲーション（逆伝播）と自動微分（Autograd）\n",
        "- （4）PyTorchの基礎： テンソルとデータ型\n",
        "- （5）データセットとデーターローダー（DataLoader）\n",
        "- （6）ディープニューラルネットのモデル定義\n",
        "- （7）学習／最適化（オプティマイザー）\n",
        "- （8）評価／精度検証"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMC_Xm6Ouqk9",
        "colab_type": "text"
      },
      "source": [
        "## ■（1）ニューロンのモデル定義"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMboLmdvjYaq",
        "colab_type": "text"
      },
      "source": [
        "###  【チェック】Pythonバージョン（※3系を使うこと）\n",
        "Colabにインストール済みのものを使う。もし2系になっている場合は、メニューバーの［ランタイム］－［ランタイムのタイプを変更］をクリックして切り替えてほしい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SziRZWCujWXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "print('Python', sys.version)\n",
        "# Python 3.6.9 (default, Nov  7 2019, 10:44:02)  …… などと表示される"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDCm1WGuu2OW",
        "colab_type": "text"
      },
      "source": [
        "###  【チェック】PyTorchバージョン\n",
        "基本的にはColabにインストール済みのものを使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EuZVjZZu663",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "print('PyTorch', torch.__version__)\n",
        "# PyTorch 1.3.1 ……などと表示される"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYTu1liZvPUq",
        "colab_type": "text"
      },
      "source": [
        "### リスト1-0　［オプション］ライブラリ「PyTorch」最新バージョンのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3ruhTNMvUQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install torch        # ライブラリ「PyTorch」をインストール\n",
        "#!pip install torchvision  # 画像／ビデオ処理のPyTorch用追加パッケージもインストール\n",
        "\n",
        "# 最新バージョンにアップグレードする場合\n",
        "!pip install --upgrade torch torchvision\n",
        "\n",
        "# バージョンを明示してアップグレードする場合\n",
        "#!pip install --upgrade torch==1.4.0 torchvision==0.5.0\n",
        "\n",
        "# 最新バージョンをインストールする場合\n",
        "#!pip install torch torchvision\n",
        "\n",
        "# バージョンを明示してインストールする場合\n",
        "#!pip install torch==1.4.0 torchvision==0.5.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDCmbMXF490y",
        "colab_type": "text"
      },
      "source": [
        "このコードのポイント：\n",
        "- 「torchvision」パッケージは本稿では使っていないが、同時にインストールしないとパッケージ関係が不整合となるため、インストールしておく必要がある\n",
        "- 実行後にランタイムを再起動する必要がある"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzWIxm9wwfpZ",
        "colab_type": "text"
      },
      "source": [
        "### ［オプション］【チェック】PyTorchバージョン（※インストール後の確認）\n",
        "バージョン1.4.0以上になっているか再度チェックする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTgmXjFewkVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "print('PyTorch', torch.__version__)\n",
        "# PyTorch 1.4.0 ……などと表示される"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3nDS-MFxo06",
        "colab_type": "text"
      },
      "source": [
        "### リスト1-1　ニューロンのモデル設計と活性化関数\n",
        "\n",
        "- ニューロンへの入力＝$(w_1 \\times X_1)+(w_2 \\times X_2)+b$\n",
        "- ニューロンからの出力＝$a((w_1 \\times X_1)+(w_2 \\times X_2)+b)$\n",
        "  - $a()$は活性化関数を意味する。つまりニューロンの入力結果を、活性化関数で変換したうえで、出力する\n",
        "  - 今回の活性化関数は、**tanh**関数とする\n",
        "- ニューロンの構造とデータ入力：座標$(X_1, X_2)$\n",
        "  - 入力の数（`INPUT_FEATURES`）は、$X_1$と$X_2$で**2つ**\n",
        "  - ニューロンの数（`OUTPUT_NEURONS`）は、**1つ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uu-jPQOxy2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
        "import torch.nn as nn  # 「ニューラルネットワーク」モジュールの別名定義\n",
        "\n",
        "# 定数（モデル定義時に必要となるもの）\n",
        "INPUT_FEATURES = 2  # 入力（特徴）の数： 2\n",
        "OUTPUT_NEURONS = 1  # ニューロンの数： 1\n",
        "\n",
        "# 変数（モデル定義時に必要となるもの）\n",
        "activation = torch.nn.Tanh()  # 活性化関数： tanh関数\n",
        "\n",
        "# 「torch.nn.Moduleクラスのサブクラス化」によるモデルの定義\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # 層（layer：レイヤー）を定義\n",
        "        self.layer1 = nn.Linear(  # Linearは「全結合層」を指す\n",
        "            INPUT_FEATURES,       # データ（特徴）の入力ユニット数\n",
        "            OUTPUT_NEURONS)       # 出力結果への出力ユニット数\n",
        "\n",
        "    def forward(self, input):\n",
        "        # フィードフォワードを定義\n",
        "        output = activation(self.layer1(input))  # 活性化関数は変数として定義\n",
        "        # 「出力＝活性化関数（第n層（入力））」の形式で記述する。\n",
        "        # 層（layer）を重ねる場合は、同様の記述を続ければよい（第3回＝後述）。\n",
        "        # 「出力（output）」は次の層（layer）への「入力（input）」に使う。\n",
        "        # 慣例では入力も出力も「x」と同じ変数名で記述する（よって以下では「x」と書く）\n",
        "        return output\n",
        "\n",
        "# モデル（NeuralNetworkクラス）のインスタンス化\n",
        "model = NeuralNetwork()\n",
        "model   # モデルの内容を出力"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9COzxIQFzqJQ",
        "colab_type": "text"
      },
      "source": [
        "このコードのポイント：\n",
        "- `torch.nn.Module`クラスを継承して独自にモデル用クラスを定義する。Pythonの「モジュール」と紛らわしいので、本稿では「`torch.nn.Module`」と表記する\n",
        "    - `__init__`関数にレイヤー（層）を定義する\n",
        "    - `forward`関数にフィードフォワード（＝活性化関数で変換しながらデータを流す処理）を実装する\n",
        "    - ちなみにバックプロパゲーション（誤差逆伝播）のための`backward`関数は自動微分機能により自動作成される（後述）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZmXJJjRA5RI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title tanh関数\n",
        "# This code will be hidden when the notebook is loaded.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "x = np.arange(-6.0, 6.0, 0.001)\n",
        "plt.plot(x, sigmoid(x), label = \"Sigmoid\")\n",
        "plt.plot(x, tanh(x), label = \"tanh\")\n",
        "plt.xlim(-6, 6)\n",
        "plt.ylim(-1.2, 1.2)\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxdAFpl2TYr7",
        "colab_type": "text"
      },
      "source": [
        "- PyTorchでは、以下の活性化関数が用意されている\n",
        "  - ELU\n",
        "  - Hardshrink\n",
        "  - Hardtanh\n",
        "  - LeakyReLU\n",
        "  - LogSigmoid\n",
        "  - MultiheadAttention\n",
        "  - PReLU\n",
        "  - ReLU（有名）\n",
        "  - ReLU6\n",
        "  - RReLU\n",
        "  - SELU\n",
        "  - CELU\n",
        "  - GELU\n",
        "  - Sigmoid（シグモイド）\n",
        "  - Softplus（ソフトプラス）\n",
        "  - Softshrink\n",
        "  - Softsign（ソフトサイン）\n",
        "  - Tanh（本稿で使用）\n",
        "  - Tanhshrink\n",
        "  - Threshold\n",
        "  - Softmin\n",
        "  - Softmax\n",
        "  - Softmax2d\n",
        "  - LogSoftmax\n",
        "  - AdaptiveLogSoftmaxWithLoss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bp5s_s611Lq",
        "colab_type": "text"
      },
      "source": [
        "### リスト1-2　パラメーター（重みとバイアス）の初期値設定\n",
        "\n",
        "- $w_1=0.6$、$w_2=-0.2$、$b=0.8$と仮定して、ニューロンのモデルを定義\n",
        "  - ※これらの値は通常は学習により決定されるが、今回は未学習なので仮の固定数値としている\n",
        "  - 重さ（$w_1$と$w_2$）は2次元配列でまとめて表記する： `weight_array`\n",
        "    - 通常は、ニューロンは複数あるので、2次元配列で表記する\n",
        "    - 複数の重みが「行」を構成し、複数のニューロンが「列」を構成する\n",
        "    - 今回は、重みが**2つ**で、ニューロンが**1つ**なので、**2行1列**で記述する\n",
        "    -  `[[ 0.6],`<br>&nbsp;&nbsp;`[-0.2]]`\n",
        "  - バイアス（$b$）は1次元配列でまとめて表記する： `bias_array`\n",
        "    - `[0.8]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe3iD9H91qON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# パラメーター（ニューロンへの入力で必要となるもの）の定義\n",
        "weight_array = nn.Parameter(\n",
        "    torch.tensor([[ 0.6,\n",
        "                   -0.2]]))  # 重み\n",
        "bias_array = nn.Parameter(\n",
        "    torch.tensor([  0.8 ]))  # バイアス\n",
        "\n",
        "# 重みとバイアスの初期値設定\n",
        "model.layer1.weight = weight_array\n",
        "model.layer1.bias = bias_array\n",
        "\n",
        "# torch.nn.Module全体の状態を辞書形式で取得\n",
        "params = model.state_dict()\n",
        "#params = list(model.parameters()) # このように取得することも可能\n",
        "params\n",
        "# 出力例：\n",
        "# OrderedDict([('layer1.weight', tensor([[ 0.6000, -0.2000]])),\n",
        "#              ('layer1.bias', tensor([0.8000]))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQOf7w_K30L9",
        "colab_type": "text"
      },
      "source": [
        "このコードのポイント：\n",
        "- モデルのパラメーターは`torch.nn.Parameter`オブジェクトとして定義する\n",
        "  - `torch.nn.Parameter`クラスのコンストラクター（厳密には`__init__`関数）には`torch.Tensor`オブジェクト（以下、テンソル）を指定する\n",
        "  - `torch.Tensor`のコンストラクターにはPythonの多次元リストを指定できる\n",
        "  - NumPyの多次元配列からのテンソルの作成や、テンソルの使い方については第2回（＝後述）\n",
        "- 重みやバイアスの初期値設定：\n",
        "  - `＜モデル名＞.＜レイヤー名＞.weight`プロパティに重みが指定できる\n",
        "  - `＜モデル名＞.＜レイヤー名＞.baias`プロパティにバイアスが指定できる\n",
        "  - 通常は「**0**」や「一様分布の**ランダムな値**」などを指定する（第3回＝後述）\n",
        "- 重みやバイアスといったパラメーターなどの`torch.nn.Module`全体の状態は、`＜モデル名＞.state_dict()`メソッドで取得できる\n",
        "  - ちなみにパラメーターを最適化で使う際は、`＜モデル名＞.parameters()`メソッドで取得する（第3回＝後述）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuu22vpliye3",
        "colab_type": "text"
      },
      "source": [
        "## ■（2）フィードフォワード（順伝播）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd1IafG72_SS",
        "colab_type": "text"
      },
      "source": [
        "### リスト2-1　フィードフォワードの実行と結果確認\n",
        "- ニューロンに、座標$(X_1, X_2)$データを入力する\n",
        "  - 通常のデータは表形式（＝2次元配列）だが、今回は$(1.0, 2.0)$という1つのデータ\n",
        "    - 1つのデータでも2次元配列（具体的には**1行2列**）で表現する必要がある"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ0vt-HN3HVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_data = torch.tensor([[1.0, 2.0]])  # 入力する座標データ（1.0、2.0）\n",
        "print(X_data)\n",
        "# tensor([[1., 2.]]) ……などと表示される\n",
        "\n",
        "y_pred = model(X_data)  # このモデルに、データを入力して、出力を得る（＝予測：predict）\n",
        "print(y_pred)\n",
        "# tensor([[0.7616]], grad_fn=<TanhBackward>) ……などと表示される"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eg6v2OEmg8x",
        "colab_type": "text"
      },
      "source": [
        "このコードのポイント：\n",
        "- フィードフォワード（順伝播）で、データ（`X_data`）を入力し、モデル（`model`）が推論した結果（`y_pred`）を出力している\n",
        "- その結果の数値は、手動で計算した値（`0.7616`）と同じになるのが確認できるはず\n",
        "- `grad_fn`属性（この例では「TanhBackward」）には、勾配（偏微分）などを計算するための関数が自動作成されている。バックプロパゲーション（逆伝播）による学習の際に利用される"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHxRfDZfjOiq",
        "colab_type": "text"
      },
      "source": [
        "### リスト2-2　動的な計算グラフの可視化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0Mo-cBPlFqB",
        "colab_type": "text"
      },
      "source": [
        "「[torchviz · PyPI](https://pypi.org/project/torchviz/)」をインストールして、動的な計算グラフ（dynamic computation graph）を可視化する。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luqbk_p352Qs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torchviz          # 初回の「torchviz」パッケージインストール時にのみ必要"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPPHf3GGhUBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchviz import make_dot  # 「torchviz」モジュールから「make_dot」関数をインポート\n",
        "make_dot(y_pred, params=dict(model.named_parameters()))\n",
        "# 引数「params」には、全パラメーターの「名前: テンソル」の辞書を指定する。\n",
        "# 「dict(model.named_parameters())」はその辞書を取得している"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9TQTF59l8AK",
        "colab_type": "text"
      },
      "source": [
        "この図のポイント：\n",
        "- 青色のボックス： 勾配を計算する必要がある、重みやバイアスなどのパラメーター。この例では`(1, 2)`が重みで、`(1)`がバイアス\n",
        "- 灰色のボックス： 勾配（偏微分）などを計算するための関数。「テンソル」データの`grad_fn`属性（この例では「TBackward」や「AddmmBackward」）に自動作成されている。バックプロパゲーション（逆伝播）による学習の際に利用される\n",
        "- 緑色のボックス： グラフ計算の開始点。`backward()`メソッドを呼び出すと、ここから逆順に計算していく。内容は灰色のボックスと同じ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFwzuitssETB",
        "colab_type": "text"
      },
      "source": [
        "## ■（3）バックプロパゲーション（逆伝播）と自動微分（Autograd）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_trGfyZBB0jX",
        "colab_type": "text"
      },
      "source": [
        "### リスト3-1　簡単な式で自動微分してみる\n",
        "\n",
        "`backward()`メソッドでバックプロパゲーション（誤差逆伝播）をさせる。ニューラルネットワークの誤差逆伝播では、「微分係数（derivative）の計算」という面倒くさい処理が待っている。ディープラーニングのライブラリは、この処理を自動化してくれるので大変便利である。この機能を「自動微分（AD： Automatic differentiation）」や「Autograd」（gradients computed automatically： 自動計算された勾配）などと呼ぶ。\n",
        "\n",
        "ちなみに詳細を知る必要はあまりないが、`torch.autograd`モジュールは厳密には「リバースモードの自動微分」機能を提供しており、vector-Jacobian product（VJP：ベクトル-ヤコビアン積）と呼ばれる計算を行うエンジンである（参考「[Autograd: Automatic Differentiation — PyTorch Tutorials 1.4.0 documentation](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)」、論文「[Automatic differentiation in PyTorch | OpenReview](https://openreview.net/forum?id=BJJsrmfCZ)」）。\n",
        "\n",
        "PyTorchの自動微分（Autograd）機能を、非常にシンプルな例で示しておく。\n",
        "\n",
        "- 計算式： $y=x^2$\n",
        "- 導関数： $\\frac{dy}{dx}=2x$ （ $y$ を $x$ で微分する）\n",
        "- 例えば $x$ が__1.0__の地点の勾配（＝接線の傾き）は__2.0__となる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-lnNEoOEYfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.tensor(1.0, requires_grad=True)  # 今回は入力に勾配（gradient）を必要とする\n",
        "# 「requires_grad」が「True」（デフォルト：False）の場合、\n",
        "# torch.autogradが入力テンソルに関するパラメーター操作（勾配）を記録するようになる\n",
        "\n",
        "#x.requires_grad_(True)  # 「requires_grad_()」メソッドで後から変更することも可能\n",
        "\n",
        "y = x ** 2     # 「yイコールxの二乗」という計算式の計算グラフを構築\n",
        "print(y)       # tensor(1., grad_fn=<PowBackward0>) ……などと表示される\n",
        "\n",
        "y.backward()   # 逆伝播の処理として、上記式から微分係数（＝勾配）を計算（自動微分：Autograd）\n",
        "\n",
        "g = x.grad     # 与えられた入力（x）によって計算された勾配の値（grad）を取得\n",
        "print(g)       # tensor(2.)  ……などと表示される\n",
        "# 計算式の微分係数（＝勾配）を計算するための導関数は「dy/dx=2x」なので、\n",
        "#「x=1.0」地点の勾配（＝接線の傾き）は「2.0」となり、出力結果は正しい。\n",
        "# 例えば「x=0.0」地点の勾配は「0.0」、「x=10.0」地点の勾配は「20.0」である"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKBUuoYOTnH4",
        "colab_type": "text"
      },
      "source": [
        "このコードのポイント：\n",
        "- PyTorchが「自動微分」機能を持つライブラリであることが確認できた\n",
        "- 出力されたテンソル（`y`）の`backward()`メソッドでバックプロパゲーション（逆伝播）を実行できる。なお、ニューラルネットワークの場合は、損失を表すテンソルの`backward()`メソッドを呼び出すことになる\n",
        "  - 出力されたテンソルの計算式（`y`）を入力したテンソル(`x`)で微分計算している\n",
        "- 計算された微分係数（＝勾配：gradient）は、入力したテンソルの`grad`プロパティで取得できる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtesDEFJoHLZ",
        "colab_type": "text"
      },
      "source": [
        "### リスト3-2　ニューラルネットワークにおける各パラメーターの勾配"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbQ1R6nKnwoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 勾配計算の前に、各パラメーター（重みやバイアス）の勾配の値（grad）をリセットしておく\n",
        "model.layer1.weight.grad = None      # 重み\n",
        "model.layer1.bias.grad = None        # バイアス\n",
        "#model.zero_grad()                   # これを呼び出しても上記と同じくリセットされる\n",
        "\n",
        "X_data = torch.tensor([[1.0, 2.0]])  # 入力データ（※再掲）\n",
        "y_pred = model(X_data)               # 出力結果（※再掲）\n",
        "y_true = torch.tensor([[1.0]])       # 正解ラベル\n",
        "\n",
        "criterion = nn.MSELoss()             # 誤差からの損失を測る「基準」＝損失関数\n",
        "loss = criterion(y_pred, y_true)     # 誤差（出力結果と正解ラベルの差）から損失を取得\n",
        "loss.backward()   # 逆伝播の処理として、勾配を計算（自動微分：Autograd）\n",
        "\n",
        "# 勾配の値（grad）は、各パラメーター（重みやバイアス）から取得できる\n",
        "print(model.layer1.weight.grad) # tensor([[-0.2002, -0.4005]])  ……などと表示される\n",
        "print(model.layer1.bias.grad)   # tensor([-0.2002])  ……などと表示される\n",
        "# ※パラメーターは「list(model.parameters())」で取得することも可能"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvSX4RtIpx8U",
        "colab_type": "text"
      },
      "source": [
        "このコードのポイント：\n",
        "- `criterion`に損失関数を代入するのが定石\n",
        "- `backward`メソッドによるバックプロパゲーション\n",
        "- この例では単純にするために1回しか処理してないが、本来はミニバッチのイテレーションや全体のエポックの回数繰り返し処理必要がある（第3回＝後述）\n",
        "- モデルにおける各パラメーター（`weight`や`bias`）の`grad`プロパティから、勾配の値は取得できる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQZPqNTzKvRI",
        "colab_type": "text"
      },
      "source": [
        "# 第2回　PyTorchのテンソル＆データ型のチートシート"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dse8lpgMzeOQ",
        "colab_type": "text"
      },
      "source": [
        "## ■（4）PyTorchの基礎： テンソルとデータ型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1twLZHGCsE_C",
        "colab_type": "text"
      },
      "source": [
        "### 表4-1　PyTorchのデータ型\n",
        "\n",
        "先ほどのリスト1-3では、`torch.tensor([0.8])`というコードでPyTorchのテンソル（`torch.Tensor`値）を作成した。PyTorchでデータや数値を扱うには、このテンソル形式にする必要がある。  \n",
        "この例で分かるように、Pythonの数値やリスト値を`torch.Tensor`値に変換するのは難しくない。  \n",
        "ここでは、テンソルを作成／変換する基本的なコードをチートシート的に書き出しておく。  \n",
        "よく分からないところがあれば、下記の公式チュートリアルの説明を参照してほしい（※Chromeの［日本語に翻訳］機能を使えば、日本語で読める）。\n",
        "- 公式チュートリアル： [What is PyTorch? — PyTorch Tutorials 1.4.0 documentation](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)\n",
        "- APIドキュメント： [torch — PyTorch master documentation](https://pytorch.org/docs/stable/torch.html#tensors)\n",
        "\n",
        "また、テンソルの中に含める数値には、PyTorch独自のデータ型（`torch.dtypes`）がある。ただし統一データ型（ある1つのテンソル内に含まれる全要素の数値は全て同じデータ型）となっており、例えば`torch.float`の場合は、全ての要素の数値が「32-bitの浮動小数点」として扱われるので注意してほしい。基本的に`torch.float`か`torch.int`しか使わない。\n",
        "\n",
        "データ型 | dtype属性への記述 | 対応するPython／NumPy（np）のデータ型\n",
        "---------|-----------------|--------------------------------\n",
        "Boolean（真偽値）|torch.bool|bool／np.bool\n",
        "8-bitの符号なし整数|torch.uint8|int／np.uint8\n",
        "8-bitの符号付き整数|torch.int8|int／np.int8\n",
        "16-bitの符号付き整数|torch.int16 ／ torch.short|int／np.uint16\n",
        "32-bitの符号付き整数|torch.int32 ／ torch.int|int／np.uint32\n",
        "64-bitの符号付き整数|torch.int64 ／ torch.long|int／np.uint64\n",
        "16-bitの浮動小数点|torch.float16 ／ torch.half|float／np.float16\n",
        "32-bitの浮動小数点|torch.float32 ／ torch.float|float／np.float32\n",
        "64-bitの浮動小数点|torch.float64 ／ torch.double|float／np.float64\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e14rqQwrdLY",
        "colab_type": "text"
      },
      "source": [
        "### リスト4-1　チートシート「テンソルの新規作成とサイズ取得／変換」"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThvwteuZBmpg",
        "colab_type": "text"
      },
      "source": [
        "以下では、シンプルにテンソルの使い方だけを示すためため、`print(x)`などの出力に関するコードは極力、省略した。コードを実行して出力を確認したい場合は、例えば`x = torch.empty(2, 3)`というコードの後に`print(x)`を追記してほしい。また、`x.size()`というコードは、`print(x.size())`のように`print`関数を適宜、自分で補ってほしい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC8YGaH5wjh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# テンソルの新規作成\n",
        "x = torch.empty(2, 3) # 2行×3列のテンソル（未初期化状態）を生成\n",
        "x = torch.rand(2, 3)  # 2行×3列のテンソル（ランダムに初期化）を生成\n",
        "x = torch.zeros(2, 3, dtype=torch.float) # 2行×3列のテンソル（0で初期化、torch.float型）を生成\n",
        "x = torch.ones(2, 3, dtype=torch.float)  # 2行×3列のテンソル（1で初期化、torch.float型）を生成\n",
        "x = torch.tensor([[0.0, 0.1, 0.2],\n",
        "                  [1.0, 1.1, 1.2]])      # 1行×2列のテンソルをPythonリスト値から作成\n",
        "\n",
        "# 既存のテンソルを使った新規作成\n",
        "# 「new_*()」パターン\n",
        "y = x.new_ones(2, 3)   # 2行×3列のテンソル（1で初期化、既存のテンソルと「同じデータ型」）を生成\n",
        "# 「*_like()」パターン # 既存のテンソルと「同じサイズ」のテンソル（1で初期化、torch.int型）を生成\n",
        "y = torch.ones_like(x, dtype=torch.int) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uhA6o7hwcgp",
        "colab_type": "text"
      },
      "source": [
        "### リスト4-2　チートシート「テンソルのサイズ取得とサイズ変更」"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN_OWJbNr30Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# テンソルサイズの取得\n",
        "x.size()               # 「torch.Size([2, 3])」のように、2行3列と出力される\n",
        "x.shape                # NumPy風の記述も可能。出力は上と同じ\n",
        "len(x)   # 行数（＝データ数）を取得する際も、NumPy風に記述することが可能\n",
        "x.ndim   # テンソルの次元数を取得する際も、NumPy風に記述が可能\n",
        "\n",
        "# テンソルのサイズ変更／形状変更\n",
        "z = x.view(3, 2)       # 3行2列に変更"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQGBrduXrvEv",
        "colab_type": "text"
      },
      "source": [
        "### リスト4-3　チートシート「テンソルの演算／計算」"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pEm5IQOsGNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# テンソルの計算操作\n",
        "x + y                  # 演算子を使う方法\n",
        "torch.add(x, y)        # 関数を使う方法\n",
        "torch.add(x, y, out=x) # outパラメーターで出力先の変数を指定可能\n",
        "x.add_(y)              # 「*_()」パターン。xを置き換えて出力する例（上記のコードと同じ処理）\n",
        "# PyTorchでは、メソッド名の最後にアンダースコア（_）がある場合（例えば「add_()」）、「テンソルの内部置き換え（in-place changes）が起こること」を意味する。\n",
        "# アンダースコア（_）がない通常の計算の場合（例えば「add()」）は、計算元のテンソル内部は変更されずに、戻り値として新たなテンソルが取得できる。"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TffYW0KgvaJb",
        "colab_type": "text"
      },
      "source": [
        "### リスト4-4　チートシート「テンソルのインデクシング／スライシング」"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBMj5dYIveaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# インデクシングやスライシング（NumPyのような添え字を使用可能）\n",
        "print(x)         # 元は、2行3列のテンソル\n",
        "x[0, 1]          # 1行2列目（※0スタート）を取得\n",
        "x[:2, 1:]        # 先頭～2行（＝0行目と1行目）×2列～末尾（＝2列目と3列目）の2行2列が抽出される"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRWf1B-TwBRn",
        "colab_type": "text"
      },
      "source": [
        "### リスト4-5　チートシート「テンソルからPython数値への変換」"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g7RxMDhwHYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# テンソルの1つの要素値を、Pythonの数値に変換\n",
        "x[0, 1].item()   # 1行2列目（※0スタート）の要素値をPythonの数値で取得"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-bw4KLMsDHc",
        "colab_type": "text"
      },
      "source": [
        "### リスト4-6　チートシート「PyTorchテンソル ←→ NumPy多次元配列値、の変換＆連携」"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiJhYlYb12Kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PyTorchテンソルを、NumPy多次元配列に変換\n",
        "b = x.numpy()    # 「numpy()」を呼び出すだけ。以下は注意点（メモリ位置の共有）\n",
        "\n",
        "# ※PyTorchテンソル側の値を変えると、NumPy多次元配列値「b」も変化する（トラックされる）\n",
        "print ('PyTorch計算→NumPy反映：')\n",
        "print(b); x.add_(y); print(b)           # PyTorch側の計算はNumPy側に反映\n",
        "print ('NumPy計算→PyTorch反映：')\n",
        "print(x); np.add(b, b, out=b); print(x) # NumPy側の計算はPyTorch側に反映\n",
        "\n",
        "# -----------------------------------------\n",
        "# NumPy多次元配列を、PyTorchテンソルに変換\n",
        "c = np.ones((2, 3), dtype=np.float64) # 2行3列の多次元配列値（1で初期化）を生成\n",
        "d = torch.from_numpy(c)  # 「torch.from_numpy()」を呼び出すだけ\n",
        "\n",
        "# ※NumPy多次元配列値を変えると、PyTorchテンソル「b」も変化する（トラックされる）\n",
        "print ('NumPy計算→PyTorch反映：')\n",
        "print(d); np.add(c, c, out=c); print(d)  # NumPy側の計算はPyTorch側に反映\n",
        "print ('PyTorch計算→NumPy反映：')\n",
        "print(c); d.add_(y); print(c)            # PyTorch側の計算はNumPy側に反映"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASri35zKvjOV",
        "colab_type": "text"
      },
      "source": [
        "### リスト4-7　チートシート「テンソルのデータ型の変換」"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4F1aW03v6OC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# データ型の変換（※変換後のテンソルには、NumPyの計算は反映されない）\n",
        "e = d.float()  # 「torch.float64」から「torch.float32」"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AWsYJ1Bvk9v",
        "colab_type": "text"
      },
      "source": [
        "### リスト4-8　チートシート「テンソル演算でのGPU利用」"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLF66iayTPhd",
        "colab_type": "text"
      },
      "source": [
        "ColabでGPUを有効にするには、メニューバーの［ランタイム］－［ランタイムのタイプを変更］をクリックして切り替えてほしい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_ABpMIEvqCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NVIDIAのGPUである「CUDA」（GPU）デバイス環境が利用可能な場合、\n",
        "# GPUを使ってテンソルの計算を行うこともできる\n",
        "if torch.cuda.is_available():              # CUDA（GPU）が利用可能な場合\n",
        "    print('CUDA（GPU）が利用できる環境')\n",
        "    print(f'CUDAデバイス数： {torch.cuda.device_count()}')\n",
        "    print(f'現在のCUDAデバイス番号： {torch.cuda.current_device()}')  # ※0スタート\n",
        "    print(f'1番目のCUDAデバイス名： {torch.cuda.get_device_name(0)}') # 例「Tesla T4」   \n",
        "\n",
        "    device = torch.device(\"cuda\")          # デフォルトのCUDAデバイスオブジェクトを取得\n",
        "    device0 = torch.device(\"cuda:0\")       # 1番目（※0スタート）のCUDAデバイスを取得\n",
        "\n",
        "    # テンソル計算でのGPUの使い方は主に3つ：\n",
        "    g = torch.ones(2, 3, device=device)    # （1）テンソル生成時のパラメーター指定\n",
        "    g = x.to(device)                       # （2）既存テンソルのデバイス変更\n",
        "    g = x.cuda(device)                     # （3）既存テンソルの「CUDA（GPU）」利用\n",
        "    f = x.cpu()                            # （3'）既存テンソルの「CPU」利用\n",
        "\n",
        "    # ※（2）の使い方で、GPUは「.to(\"cuda\")」、CPUは「.to(\"cpu\")」と書いてもよい\n",
        "    g = x.to(\"cuda\")\n",
        "    f = x.to(\"cpu\")\n",
        "\n",
        "    # ※（3）の引数は省略することも可能\n",
        "    g = x.cuda()\n",
        "\n",
        "    # 「torch.nn.Module」オブジェクト（model）全体でのGPU／CPUの切り替え\n",
        "    model.cuda()  # モデルの全パラメーターとバッファーを「CUDA（GPU）」に移行する\n",
        "    model.cpu()   # モデルの全パラメーターとバッファーを「CPU」に移行する\n",
        "else:\n",
        "    print('CUDA（GPU）が利用できない環境')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W2oc9K0KxGz",
        "colab_type": "text"
      },
      "source": [
        "# 第3回　PyTorchによるディープラーニング実装手順の基本"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVfZeXrgYg7m",
        "colab_type": "text"
      },
      "source": [
        "## ■（5）データセットとデーターローダー（DataLoader）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOb1W3i57oGe",
        "colab_type": "text"
      },
      "source": [
        "「[第1回　初めてのニューラルネットワーク実装、まずは準備をしよう ― 仕組み理解×初実装（前編）：TensorFlow 2＋Keras（tf.keras）入門 - ＠IT](https://www.atmarkit.co.jp/ait/articles/1909/19/news026.html)」の記事と同じように、シンプルな座標点データを生成して使う。使い方は、前述の記事を参照してほしい。\n",
        "\n",
        "なお、座標点データは、「[ニューラルネットワーク Playground - Deep Insider](https://deepinsider.github.io/playground/)」（以下、Playground）と同じ生成仕様となっている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2fU5kIiRher",
        "colab_type": "text"
      },
      "source": [
        "### リスト5-1　座標点データの生成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnL886VJR-HB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 座標点データを生成するライブラリのインストール\n",
        "!pip install playground-data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M_-s3ooRmVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# playground-dataライブラリのplygdataパッケージを「pg」という別名でインポート\n",
        "import plygdata as pg\n",
        "\n",
        "# 設定値を定数として定義\n",
        "PROBLEM_DATA_TYPE = pg.DatasetType.ClassifyCircleData # 問題種別：「分類（Classification）」、データ種別：「円（CircleData）」を選択\n",
        "TRAINING_DATA_RATIO = 0.5  # データの何％を訓練【Training】用に？ (残りは精度検証【Validation】用) ： 50％\n",
        "DATA_NOISE = 0.0           # ノイズ： 0％\n",
        "\n",
        "# 定義済みの定数を引数に指定して、データを生成する\n",
        "data_list = pg.generate_data(PROBLEM_DATA_TYPE, DATA_NOISE)\n",
        "\n",
        "# データを「訓練用」と「精度検証用」を指定の比率で分割し、さらにそれぞれを「データ（X）」と「教師ラベル（y）」に分ける\n",
        "X_train, y_train, X_valid, y_valid = pg.split_data(data_list, training_size=TRAINING_DATA_RATIO)\n",
        "\n",
        "# データ分割後の各変数の内容例として、それぞれ5件ずつ出力\n",
        "print('X_train:'); print(X_train[:5])\n",
        "print('y_train:'); print(y_train[:5])\n",
        "print('X_valid:'); print(X_valid[:5])\n",
        "print('y_valid:'); print(y_valid[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYfp-w3t-BjX",
        "colab_type": "text"
      },
      "source": [
        "### リスト5-2　データセットとデーターローダーの作成\n",
        "\n",
        "PyTorchにはミニバッチを簡単に扱うための`DataLoader`クラスが用意されている。このクラスを利用するには、既存のデータや教師ラベルといったテンソルを1つの`TensorDataset`にまとめる必要がある。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Rj8ASo8ICO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# データ関連のユーティリティクラスをインポート\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
        "\n",
        "# 定数（学習方法設計時に必要となるもの）\n",
        "BATCH_SIZE = 15  # バッチサイズ： 15（Playgroundの選択肢は「1」～「30」）\n",
        "\n",
        "# NumPy多次元配列からテンソルに変換し、データ型は`float`に変換する\n",
        "t_X_train = torch.from_numpy(X_train).float()\n",
        "t_y_train = torch.from_numpy(y_train).float()\n",
        "t_X_valid = torch.from_numpy(X_valid).float()\n",
        "t_y_valid = torch.from_numpy(y_valid).float()\n",
        "\n",
        "# 「データ（X）」と「教師ラベル（y）」を、1つの「データセット（dataset）」にまとめる\n",
        "dataset_train = TensorDataset(t_X_train, t_y_train)  # 訓練用\n",
        "dataset_valid = TensorDataset(t_X_valid, t_y_valid)  # 精度検証用\n",
        "\n",
        "# ミニバッチを扱うための「データローダー（loader）」（訓練用と精度検証用）を作成\n",
        "loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "loader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbevi-yv_iRg",
        "colab_type": "text"
      },
      "source": [
        "このコードのポイント：\n",
        "- バッチサイズは学習時に扱うデータ単位であるが、`DataLoader`が「ミニバッチ」に関係するため、この段階で定義しておく必要がある\n",
        "- `DataLoader`クラスのコンストラクター引数`shuffle`で、データをシャッフルするか（**True**）しないか（**False**）を指定できる。今回はシャッフルしている"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFwgzpvAOsQo",
        "colab_type": "text"
      },
      "source": [
        "## ■（6）ディープニューラルネットのモデル定義"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w3l4TB9CgvY",
        "colab_type": "text"
      },
      "source": [
        "### リスト6-1　「1」か「-1」に分類するための「出力の離散化」\n",
        "- 今回のニューラルネットワークでは出力された確率値を、「**1**」か「**-1**」の2クラス分類値に離散化する\n",
        "  - 具体的には、0.0未満／0.0以上を-1.0／1.0にスケール変換する\n",
        "- そのための独自関数`discretize`を定義する\n",
        "- さらに、モデル内で扱いやすいように`torch.nn.Module`化も行っておく（※本稿では使用しない）\n",
        "- `discretize`関数は後述の「リスト7-3　1回分の「訓練（学習）」と「評価」の処理」で使用する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRN9Ox7UCtEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
        "import torch.nn as nn  # 「ニューラルネットワーク」モジュールの別名定義\n",
        "\n",
        "# 離散化を行う単なる関数\n",
        "def discretize(proba):\n",
        "    '''\n",
        "    実数の確率値を「1」か「-1」の2クラス分類値に離散化する。\n",
        "    閾値は「0.0以上」か「未満」か。データ型は「torch.float」を想定。\n",
        "  \n",
        "    Examples:\n",
        "        >>> proba = torch.tensor([-0.5, 0.0, 0.5], dtype=torch.float)\n",
        "        >>> binary = discretize(proba)\n",
        "    '''\n",
        "    threshold = torch.Tensor([0.0]) # -1か1かを分ける閾値を作成\n",
        "    discretized = (proba >= threshold).float() # 閾値未満で0、以上で1に変換\n",
        "    return discretized * 2 - 1.0 # 2倍して-1.0することで、0／1を-1.0／1.0にスケール変換\n",
        "\n",
        "# discretize関数をモデルで簡単に使用できるようにするため、\n",
        "# PyTorchの「torch.nn.Module」を継承したクラスラッパーも作成した\n",
        "class Discretize(nn.Module):\n",
        "    '''\n",
        "    実数の確率値を「1」か「-1」の2クラス分類値に離散化する。\n",
        "    閾値は「0.0以上」か「未満」か。データ型は「torch.float」を想定。\n",
        "  \n",
        "    Examples:\n",
        "        >>> d = Discretize()\n",
        "        >>> proba = torch.tensor([-0.5, 0.0, 0.5], dtype=torch.float)\n",
        "        >>> binary = d(proba)\n",
        "    '''        \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # forward()メソッドは、基本クラス「torch.nn.Module」の__call__メソッドからも呼び出されるため、\n",
        "    # Discretizeオブジェクトを関数のように使える（例えば上記の「d(proba)」）\n",
        "    def forward(self, proba):\n",
        "        return discretize(proba) # 上記の関数を呼び出すだけ\n",
        "\n",
        "# 関数の利用をテスト\n",
        "proba = torch.tensor([-0.5, 0.0, 0.5], dtype=torch.float)  # 確率値の例\n",
        "binary = discretize(proba)  # 2クラス分類（binary classification）値に離散化\n",
        "binary  # tensor([-1.,  1.,  1.]) …… などと表示される"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFvuSutuCgfq",
        "colab_type": "text"
      },
      "source": [
        "このコードのポイント：\n",
        "- PyTrochのニューラルネットワークの基本を解説する内容ではないので、ざっと流して読めば十分（コメントを参考にしてほしい）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VT16L_p1o1R",
        "colab_type": "text"
      },
      "source": [
        "### リスト6-2　ディープニューラルネットワークのモデル設計\n",
        "- 入力の数（`INPUT_FEATURES`）は、$X_1$と$X_2$で**2つ**\n",
        "- 隠れ層のレイヤー数は、**2つ**\n",
        "  - 隠れ層にある1つ目のニューロンの数（`LAYER1_NEURONS`）は、**3つ**\n",
        "  - 隠れ層にある2つ目のニューロンの数（`LAYER2_NEURONS`）は、**3つ**\n",
        "- 出力層にあるニューロンの数（`OUTPUT_RESULTS`）は、**1つ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28aZTzo_lFGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
        "import torch.nn as nn  # 「ニューラルネットワーク」モジュールの別名定義\n",
        "\n",
        "# 定数（モデル定義時に必要となるもの）\n",
        "INPUT_FEATURES = 2      # 入力（特徴）の数： 2\n",
        "LAYER1_NEURONS = 3      # ニューロンの数： 3\n",
        "LAYER2_NEURONS = 3      # ニューロンの数： 3\n",
        "OUTPUT_RESULTS = 1      # 出力結果の数： 1\n",
        "\n",
        "# 変数（モデル定義時に必要となるもの）\n",
        "activation = torch.nn.Tanh()  # 活性化関数（隠れ層用）： tanh関数（変更可能）\n",
        "act_output = torch.nn.Tanh()  # 活性化関数（出力層用）： tanh関数（固定）\n",
        "\n",
        "# torch.nn.Moduleによるモデルの定義\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # 隠れ層：1つ目のレイヤー（layer）\n",
        "        self.layer1 = nn.Linear(\n",
        "            INPUT_FEATURES,                # 入力ユニット数（＝入力層）\n",
        "            LAYER1_NEURONS)                # 次のレイヤーの出力ユニット数\n",
        "\n",
        "        # 隠れ層：2つ目のレイヤー（layer）\n",
        "        self.layer2 = nn.Linear(\n",
        "            LAYER1_NEURONS,                # 入力ユニット数\n",
        "            LAYER2_NEURONS)                # 次のレイヤーへの出力ユニット数\n",
        "\n",
        "        # 出力層\n",
        "        self.output = nn.Linear(\n",
        "            LAYER2_NEURONS,                # 入力ユニット数\n",
        "            OUTPUT_RESULTS)                # 出力結果への出力ユニット数\n",
        "\n",
        "    def forward(self, x):\n",
        "        # フィードフォワードを定義\n",
        "        # 「出力＝活性化関数（第n層（入力））」の形式で記述\n",
        "        x = activation(self.layer1(x))  # 活性化関数は変数として定義\n",
        "        x = activation(self.layer2(x))  # 同上\n",
        "        x = act_output(self.output(x))  # ※活性化関数は「tanh」固定\n",
        "        return x\n",
        "\n",
        "# モデル（NeuralNetworkクラス）のインスタンス化\n",
        "model = NeuralNetwork()\n",
        "model   # モデルの内容を出力"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8smtM4J4F7x-",
        "colab_type": "text"
      },
      "source": [
        "このコードのポイント：\n",
        "- 基本的な内容は、前掲の「リスト1-1　ニューロンのモデル設計」と同じ\n",
        "- 層が1つから、「隠れ層：2＋出力層：1」の3つに拡張されている\n",
        "- 例えば「LAYER1_NEURONS」に着目すると、1つ目のレイヤーにおける「出力ユニット数」であり、2つ目のレイヤーの「入力ユニット数」でもあるので、全く同じものが指定されている\n",
        "- フィードフォワード時のデータが変換されていく流れは、`forward`メソッド内に分かりやすく定義されている"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiaaAwr4jDNG",
        "colab_type": "text"
      },
      "source": [
        "## ■（7）学習／最適化（オプティマイザー）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95b3Pe4uOhgx",
        "colab_type": "text"
      },
      "source": [
        "### リスト7-1　オプティマイザー（最適化用オブジェクト）の作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMaORFyxw31H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim   # 「最適化」モジュールの別名定義\n",
        "\n",
        "# 定数（学習方法設計時に必要となるもの）\n",
        "LEARNING_RATE = 0.03   # 学習率： 0.03\n",
        "REGULARIZATION = 0.03  # 正則化率： 0.03\n",
        "\n",
        "# オプティマイザーを作成（パラメーターと学習率も指定）\n",
        "optimizer = optim.SGD(           # 最適化アルゴリズムに「SGD」を選択\n",
        "    model.parameters(),          # 最適化で更新対象のパラメーター（重みやバイアス）\n",
        "    lr=LEARNING_RATE,            # 更新時の学習率\n",
        "    weight_decay=REGULARIZATION) # L2正則化（※不要な場合は0か省略）"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpjXYAQJYcHG",
        "colab_type": "text"
      },
      "source": [
        "このコードのポイント：\n",
        "- この例では「SGD」（Stochastic Gradient Descent： 確率的勾配降下法）を選択\n",
        "- パラメーター（重みやバイアス）と、学習率、正則化率を引数に指定\n",
        "- 正則化（regularization）は「L2」に相当する。あまり使わない「L1」は基本機能に含まれていない\n",
        "- `torch.optim.SGD`を含めて以下が使用可能\n",
        "  - Adadelta\n",
        "  - Adagrad\n",
        "  - Adam（有名）\n",
        "  - AdamW\n",
        "  - SparseAdam\n",
        "  - Adamax\n",
        "  - ASGD\n",
        "  - LBFGS\n",
        "  - RMSprop\n",
        "  - Rprop\n",
        "  - SGD（確率的勾配降下法）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K48EyWqY8QCw",
        "colab_type": "text"
      },
      "source": [
        "### リスト7-2　損失関数の定義\n",
        "定義した損失関数は次のリスト7-3で利用する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8luTeeRNBNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 変数（学習方法設計時に必要となるもの）\n",
        "criterion = nn.MSELoss()  # 損失関数：平均二乗誤差"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_Bj8n7PQmvf",
        "colab_type": "text"
      },
      "source": [
        "このコードのポイント：\n",
        "- `criterion`は慣例の変数名。誤差からの損失を測る「基準（criterion）」を意味する\n",
        "- `nn.MSELoss`も含めて以外が使用可能\n",
        "  - L1Loss（MAE：Mean Absolute Error、平均絶対誤差）\n",
        "  - MSELoss（MSE：Mean Squared Error、平均二乗誤差）\n",
        "  - CrossEntropyLoss（交差エントロピー誤差： クラス分類）\n",
        "  - CTCLoss\n",
        "  - NLLLoss\n",
        "  - PoissonNLLLoss\n",
        "  - KLDivLoss\n",
        "  - BCELoss\n",
        "  - BCEWithLogitsLoss\n",
        "  - MarginRankingLoss\n",
        "  - HingeEmbeddingLoss\n",
        "  - MultiLabelMarginLoss\n",
        "  - SmoothL1Loss\n",
        "  - SoftMarginLoss\n",
        "  - MultiLabelSoftMarginLoss\n",
        "  - CosineEmbeddingLoss\n",
        "  - MultiMarginLoss\n",
        "  - TripletMarginLoss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aehaIlYEhSWe",
        "colab_type": "text"
      },
      "source": [
        "### リスト7-3　1回分の「訓練（学習）」と「評価」の処理\n",
        "`train_step`関数に訓練の内容を、`valid_step`関数に評価の内容を記述する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC8RzeLxapFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(train_X, train_y):\n",
        "    # 訓練モードに設定\n",
        "    model.train()\n",
        "\n",
        "    # フィードフォワードで出力結果を取得\n",
        "    #train_X                # 入力データ\n",
        "    pred_y = model(train_X) # 出力結果\n",
        "    #train_y                # 正解ラベル\n",
        "\n",
        "    # 出力結果と正解ラベルから損失を計算し、勾配を求める\n",
        "    optimizer.zero_grad()   # 勾配を0で初期化（※累積してしまうため要注意）\n",
        "    loss = criterion(pred_y, train_y)     # 誤差（出力結果と正解ラベルの差）から損失を取得\n",
        "    loss.backward()   # 逆伝播の処理として勾配を計算（自動微分）\n",
        "\n",
        "    # 勾配を使ってパラメーター（重みとバイアス）を更新\n",
        "    optimizer.step()  # 指定されたデータ分の最適化を実施\n",
        "\n",
        "    # 正解率を算出\n",
        "    with torch.no_grad(): # 勾配は計算しないモードにする\n",
        "        discr_y = discretize(pred_y)         # 確率値から「-1」／「1」に変換\n",
        "        acc = (discr_y == train_y).sum()     # 正解数を計算する\n",
        "\n",
        "    # 損失と正解数をタプルで返す\n",
        "    return (loss.item(), acc.item())  # ※item()=Pythonの数値\n",
        "\n",
        "def valid_step(valid_X, valid_y):\n",
        "    # 評価モードに設定（※dropoutなどの挙動が評価用になる）\n",
        "    model.eval()\n",
        "    \n",
        "    # フィードフォワードで出力結果を取得\n",
        "    #valid_X                # 入力データ\n",
        "    pred_y = model(valid_X) # 出力結果\n",
        "    #valid_y                # 正解ラベル\n",
        "\n",
        "    # 出力結果と正解ラベルから損失を計算\n",
        "    loss = criterion(pred_y, valid_y)     # 誤差（出力結果と正解ラベルの差）から損失を取得\n",
        "    # ※評価時は勾配を計算しない\n",
        "\n",
        "    # 正解率を算出\n",
        "    with torch.no_grad(): # 勾配は計算しないモードにする\n",
        "        discr_y = discretize(pred_y)     # 確率値から「-1」／「1」に変換\n",
        "        acc = (discr_y == valid_y).sum() # 正解数を合計する\n",
        "\n",
        "    # 損失と正解数をタプルで返す\n",
        "    return (loss.item(), acc.item())  # ※item()=Pythonの数値"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIkBn4GQiKaM",
        "colab_type": "text"
      },
      "source": [
        "このコードのポイント：\n",
        "- `model.eval()`メソッドを呼び出すと、評価（推論）モードとなり、（今回は使っていないが）BatchNormやDropoutなどの挙動が評価用になる。通常は、訓練モード（`model.train()`メソッド）になっている\n",
        "- `train_step`関数の処理内容： 「訓練モードの設定」「フィードフォワードで出力結果の取得」「出力結果と正解ラベルから損失および勾配の計算」「勾配を使ってパラメーター（重みとバイアス）の更新」「正解率の算出」\n",
        "- `valid_step`関数の処理内容： 「評価モードの設定」「フィードフォワードで出力結果の取得」「出力結果と正解ラベルから損失の計算」「正解率の算出」\n",
        "- フィードフォワードは、前掲の「リスト2-1　フィードフォワードの実行と結果確認」で説明済み\n",
        "- `with torch.no_grad():`の配下のテンソル計算のコードには自動微分用の勾配が生成されなくなり、メモリ使用量軽減やスピードアップなどの効果がある\n",
        "- `discretize`関数は前掲の『リスト6-1　「1」か「-1」に分類するための「出力の離散化」』で説明した"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w-uPfRo22UT",
        "colab_type": "text"
      },
      "source": [
        "### リスト7-4　「訓練」と「評価」をバッチサイズ単位でエポック回繰り返す\n",
        "`train_step`関数で訓練を、`valid_step`関数で評価を実行する。早期終了は基本機能ではないので今回は説明しない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG-yEd_qdETL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# パラメーター（重みやバイアス）の初期化を行う関数の定義\n",
        "def init_parameters(layer):\n",
        "    if type(layer) == nn.Linear:\n",
        "        nn.init.xavier_uniform_(layer.weight) # 重みを「一様分布のランダム値」に初期化\n",
        "        layer.bias.data.fill_(0.0)            # バイアスを「0」に初期化\n",
        "\n",
        "# 学習の前にパラメーター（重みやバイアス）を初期化する\n",
        "model.apply(init_parameters)\n",
        "\n",
        "# 定数（学習／評価時に必要となるもの）\n",
        "EPOCHS = 100             # エポック数： 100\n",
        "\n",
        "# 変数（学習／評価時に必要となるもの）\n",
        "avg_loss = 0.0           # 「訓練」用の平均「損失値」\n",
        "avg_acc = 0.0            # 「訓練」用の平均「正解率」\n",
        "avg_val_loss = 0.0       # 「評価」用の平均「損失値」\n",
        "avg_val_acc = 0.0        # 「評価」用の平均「正解率」\n",
        "\n",
        "# 損失の履歴を保存するための変数\n",
        "train_history = []\n",
        "valid_history = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # forループ内で使う変数と、エポックごとの値リセット\n",
        "    total_loss = 0.0     # 「訓練」時における累計「損失値」\n",
        "    total_acc = 0.0      # 「訓練」時における累計「正解数」\n",
        "    total_val_loss = 0.0 # 「評価」時における累計「損失値」\n",
        "    total_val_acc = 0.0  # 「評価」時における累計「正解数」\n",
        "    total_train = 0      # 「訓練」時における累計「データ数」\n",
        "    total_valid = 0      # 「評価」時における累計「データ数」\n",
        "\n",
        "    for train_X, train_y in loader_train:\n",
        "        # 【重要】1ミニバッチ分の「訓練」を実行\n",
        "        loss, acc = train_step(train_X, train_y)\n",
        "\n",
        "        # 取得した損失値と正解率を累計値側に足していく\n",
        "        total_loss += loss          # 訓練用の累計損失値\n",
        "        total_acc += acc            # 訓練用の累計正解数\n",
        "        total_train += len(train_y) # 訓練データの累計数\n",
        "            \n",
        "    for valid_X, valid_y in loader_valid:\n",
        "        # 【重要】1ミニバッチ分の「評価（精度検証）」を実行\n",
        "        val_loss, val_acc = valid_step(valid_X, valid_y)\n",
        "\n",
        "        # 取得した損失値と正解率を累計値側に足していく\n",
        "        total_val_loss += val_loss  # 評価用の累計損失値\n",
        "        total_val_acc += val_acc    # 評価用の累計正解数\n",
        "        total_valid += len(valid_y) # 訓練データの累計数\n",
        "\n",
        "    # ミニバッチ単位で累計してきた損失値や正解率の平均を取る\n",
        "    n = epoch + 1                             # 処理済みのエポック数\n",
        "    avg_loss = total_loss / n                 # 訓練用の平均損失値\n",
        "    avg_acc = total_acc / total_train         # 訓練用の平均正解率\n",
        "    avg_val_loss = total_val_loss / n         # 訓練用の平均損失値\n",
        "    avg_val_acc = total_val_acc / total_valid # 訓練用の平均正解率\n",
        "\n",
        "    # グラフ描画のために損失の履歴を保存する\n",
        "    train_history.append(avg_loss)\n",
        "    valid_history.append(avg_val_loss)\n",
        "\n",
        "    # 損失や正解率などの情報を表示\n",
        "    print(f'[Epoch {epoch+1:3d}/{EPOCHS:3d}]' \\\n",
        "          f' loss: {avg_loss:.5f}, acc: {avg_acc:.5f}' \\\n",
        "          f' val_loss: {avg_val_loss:.5f}, val_acc: {avg_val_acc:.5f}')\n",
        "\n",
        "print('Finished Training')\n",
        "print(model.state_dict())  # 学習後のパラメーターの情報を表示"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F8wBgA6LYmk",
        "colab_type": "text"
      },
      "source": [
        "このコードのポイント：\n",
        "- 重要なのは、「リスト7-3　1回分の「訓練（学習）」と「評価」の処理」で定義した`train_step`関数と`valid_step`関数の呼び出しだけである\n",
        "- 1つ目のforループでエポックを回している\n",
        "  - 2つ目のforループでバッチ単位分のデータを処理に渡している（ミニバッチ処理）\n",
        "- たくさんの変数があって長いが、どれも表示用の損失や正解率を計算するための処理であり、ニューラルネットワークの本質ではない"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz1U1sOTaDiN",
        "colab_type": "text"
      },
      "source": [
        "## ■（8）評価／精度検証"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl7Cau6jGqF6",
        "colab_type": "text"
      },
      "source": [
        "### リスト8-1　損失値の推移グラフ描画"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWlji88SdXpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 学習結果（損失）のグラフを描画\n",
        "epochs = len(train_history)\n",
        "plt.plot(range(epochs), train_history, marker='.', label='loss (Training data)')\n",
        "plt.plot(range(epochs), valid_history, marker='.', label='loss (validation data)')\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VSsuIVHLTn4",
        "colab_type": "text"
      },
      "source": [
        "このコードのポイント：\n",
        "- `train_history`と`valid_history`は、前述の「リスト7-4　「訓練」と「評価」をバッチサイズ単位でエポック回繰り返す」で記録しておいた損失の履歴である\n",
        "- プロットする方法はいたって普通なので、特に本稿では説明しない"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5KufJ89cs2d",
        "colab_type": "text"
      },
      "source": [
        "# お疲れさまでした。第1回～第3回は修了です。\n",
        "\n",
        "- 「第1回　難しくない！ PyTorchでニューラルネットワークの基本」\n",
        "- 「第2回　PyTorchのテンソル＆データ型のチートシート」\n",
        "- 「第3回　PyTorchによるディープラーニング実装手順の基本」\n"
      ]
    }
  ]
}